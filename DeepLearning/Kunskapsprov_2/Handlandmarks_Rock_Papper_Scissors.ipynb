{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import img_to_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the hand positions\n",
    "def hand_possition_class(landmarks):  \n",
    "    # giving names to a few landmarks for readability\n",
    "    thumb_tip = landmarks[mp_hands.HandLandmark.THUMB_TIP]\n",
    "    index_tip = landmarks[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    middle_tip = landmarks[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "    ring_tip = landmarks[mp_hands.HandLandmark.RING_FINGER_TIP]\n",
    "    pinky_tip = landmarks[mp_hands.HandLandmark.PINKY_TIP]\n",
    "    \n",
    "    thumb_mcp = landmarks[mp_hands.HandLandmark.THUMB_MCP]\n",
    "    index_mcp = landmarks[mp_hands.HandLandmark.INDEX_FINGER_MCP]\n",
    "    middle_mcp = landmarks[mp_hands.HandLandmark.MIDDLE_FINGER_MCP]\n",
    "    ring_mcp = landmarks[mp_hands.HandLandmark.RING_FINGER_MCP]\n",
    "    pinky_mcp = landmarks[mp_hands.HandLandmark.PINKY_MCP]\n",
    "   \n",
    "     \n",
    "  # Calculate distances between tip and mcp\n",
    "    dist_thumb_index_tip = np.linalg.norm(\n",
    "        np.array([thumb_tip.x, thumb_tip.y]) - \n",
    "        np.array([index_tip.x, index_tip.y])\n",
    "        )\n",
    "    \n",
    "    dist_index_mcp_tip = np.linalg.norm(\n",
    "        np.array([index_tip.x, index_tip.y]) - \n",
    "        np.array([index_mcp.x, index_mcp.y])\n",
    "        )\n",
    "    \n",
    "    dist_middle_mcp_tip = np.linalg.norm(\n",
    "        np.array([middle_tip.x, middle_tip.y]) - \n",
    "        np.array([middle_mcp.x, middle_mcp.y])\n",
    "        )\n",
    "    \n",
    "    dist_ring_mcp_tip = np.linalg.norm(\n",
    "        np.array([ring_tip.x, ring_tip.y]) - \n",
    "        np.array([ring_mcp.x, ring_mcp.y])\n",
    "        )\n",
    "    \n",
    "    dist_pinky_mcp_tip = np.linalg.norm(\n",
    "        np.array([pinky_tip.x, pinky_tip.y]) - \n",
    "        np.array([pinky_mcp.x, pinky_mcp.y])\n",
    "        )\n",
    "    \n",
    "    \n",
    " # Classification of rock papper scissors\n",
    "    if (dist_index_mcp_tip < 0.1 and \n",
    "        dist_middle_mcp_tip < 0.1 and \n",
    "        dist_ring_mcp_tip < 0.1 and \n",
    "        dist_pinky_mcp_tip < 0.1):\n",
    "        return \"Rock\"\n",
    "    elif (dist_index_mcp_tip > 0.1 and \n",
    "          dist_middle_mcp_tip > 0.1 and \n",
    "          dist_ring_mcp_tip > 0.1 and \n",
    "          dist_pinky_mcp_tip > 0.1):\n",
    "        return \"Paper\"\n",
    "    else:\n",
    "        return \"Scissors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game rules\n",
    "def determine_winner(hand1, hand2):\n",
    "    if hand1 == hand2:\n",
    "        return \"Tie\"\n",
    "    if ((hand1 == \"Rock\" and hand2 == \"Scissors\") or \n",
    "        (hand1 == \"Scissors\" and hand2 == \"Paper\") or \n",
    "        (hand1 == \"Paper\" and hand2 == \"Rock\")):\n",
    "        return \"Player 1 Wins!\"\n",
    "    return \"Player 2 Wins!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Detect face\n",
    "face_classifier = cv2.CascadeClassifier(r'C:\\Users\\Hanss\\Documents\\Utbildning\\Deep Learning\\Emotion_Detection_CNN\\haarcascade_frontalface_default.xml')\n",
    "emotion_classifier = load_model(r\"C:\\Users\\Hanss\\Documents\\Utbildning\\Deep Learning\\emotion_model.keras\")\n",
    "\n",
    "# Detect and track hands, must be 50% sure that it is a hand \n",
    "# and 50% sure it is tracking the same hand. \n",
    "with mp_hands.Hands(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Skipped frame\")\n",
    "            continue\n",
    "\n",
    "        # Gives a mirror image for the user, easier for user to understand\n",
    "        image = cv2.flip(image, 1)\n",
    "\n",
    "        # Open CV uses BGR color format changes to RGB color \n",
    "        # format that mediapipe uses.\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        \n",
    "        results = hands.process(image_rgb)\n",
    "        \n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_classifier.detectMultiScale(gray)\n",
    "        \n",
    "        image.flags.writeable = True\n",
    "        detected_hand_list = []\n",
    "\n",
    "        # Enhancing the visual understanding of hand positions in the processed image.\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                \n",
    "                hand_posistion = hand_possition_class(hand_landmarks.landmark)\n",
    "                detected_hand_list.append(hand_posistion)\n",
    "\n",
    "                # Displaying the classification\n",
    "                cv2.putText(image, hand_posistion, \n",
    "                            (int(hand_landmarks.landmark[0].x * image.shape[1] + 10), \n",
    "                             int(hand_landmarks.landmark[0].y * image.shape[0])),\n",
    "                            cv2.FONT_HERSHEY_TRIPLEX, 0.8, (128, 0, 128), 1, cv2.LINE_4\n",
    "                            )\n",
    "        # Displaying the winner\n",
    "        if len(detected_hand_list) == 2:\n",
    "            outcome = determine_winner(detected_hand_list[0], detected_hand_list[1])\n",
    "            middle_x = (image.shape[1] // 2) - (14 // 2)\n",
    "            cv2.putText(image, outcome, ((middle_x, 50)),\n",
    "                        cv2.FONT_HERSHEY_TRIPLEX, 0.8, (255, 0, 0), 1, cv2.LINE_8)\n",
    "\n",
    "    \n",
    "        cv2.imshow('Rock Paper Scissors', image)\n",
    "\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VSUtbildning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
